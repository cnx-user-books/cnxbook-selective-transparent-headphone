<document xmlns="http://cnx.rice.edu/cnxml">
  <title>Implementation and Experimental Results</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m52253</md:content-id>
  <md:title>Implementation and Experimental Results</md:title>
  <md:abstract>This is our implementation of neural network and Blind Source Separation and their respective results</md:abstract>
  <md:uuid>efa0d627-fb0b-48d4-928d-f10f80b55de0</md:uuid>
</metadata>

<content>
    <para id="import-auto-id2216584">
      <emphasis effect="bold">Implementation of Blind Source Separation:</emphasis>
    </para>
    <para id="import-auto-id1168899942274">When implementing the Blind Source Separation algorithm, we chose the following parameters,</para>


<figure id="parameter">
  
  <media id="para_table" alt="para">
    <image mime-type="image/jpeg" src="../../media/parameters.jpg" height="300"/>
  </media>
  <caption>
     Input Parameter Into The System
  </caption>
</figure>

    <para id="import-auto-id1168891232252">
      <emphasis effect="bold">Result of our Blind Source Separation:</emphasis>
    </para>

<figure id="oberved1">
  
  <media id="observed1" alt="observe1">
    <image mime-type="image/png" src="../../media/observed1.png" height="300"/>
  </media>
  <caption>
     Mixed Signal Observed at 1<sup>st</sup> Microphone
  </caption>
</figure>

<figure id="oberved2">
  
  <media id="observed2" alt="observe2">
    <image mime-type="image/png" src="../../media/observed2.png" height="300"/>
  </media>
  <caption>
     Mixed Signal Observed at 2<sup>nd</sup> Microphone
  </caption>
</figure>

<figure id="oberved3">
  
  <media id="observed3" alt="observe3">
    <image mime-type="image/png" src="../../media/source1.png" height="300"/>
  </media>
  <caption>
     Estimated Source Signal One-human speech
  </caption>
</figure>

<figure id="oberved4">
  
  <media id="observed4" alt="observe4">
    <image mime-type="image/png" src="../../media/source2.png" height="300"/>
  </media>
  <caption>
     Estimated Source Signal Two-Instrumental Music
  </caption>
</figure>
    <para id="import-auto-id7183815">Figure 2 and Figure 3 show the mixed signals observed at the 1<sup>st</sup> microphone and the 2<sup>nd</sup> microphone respectively. The mixed signal is a recording of a person counting from zero to ten with instrumental music playing in the background. The intermittent spikes in the mixed signal represents the person counting while the continuous, small-amplitude signal represents the instrumental music. </para>
    <para id="import-auto-id1168894654933">After applying the Blind Source Separation algorithm, we obtain two independent signals shown in Figure 4 and Figure 5 respectively. These two signals are very distinct from each other. The output signal 1 shown in Figure 4 consists mainly of the person counting whereas the output signal 2 shown in Figure 5 is primarily the instrumental music. </para>
    <para id="import-auto-id1168894018851">
      <emphasis effect="bold">Implementation of Neural Network:</emphasis>
    </para>
    <para id="import-auto-id1168896304490">Since we need our neural network to perform binary classification, we used 512 nodes for the input layer, 25 nodes for the hidden layer, and 2 nodes for the output layer. </para>
    <para id="import-auto-id1168900280663">We used 10,000 positive training examples and 10,000 negative training examples to train our neural network. The positive training examples mainly consist of audio file containing examples of human speech such as news broadcasting. The negative training examples mainly consist of examples of non-human speech such as instrumental music. </para>
    <para id="import-auto-id1168895590730">After taking 512 point Short-Time Fourier Transform, we place our training sets into a 20,000-by-512 matrix where each row represents a single training example. Then we used the backpropagation and gradient descent algorithm described in the section above to train our neural network. We used 70% of the training examples for training, 15% of the training examples for validation, and another 15% for testing. </para>
    <para id="import-auto-id2938275">
      <emphasis effect="bold">Result of Our Training of Neural Network:</emphasis>
    </para>
    <para id="import-auto-id1168896483006">After employing backpropagation algorithm and gradient descent algorithm to train our neural network, we get the following test results:</para>
<figure id="nntrain">
  
  <media id="train" alt="train">
    <image mime-type="image/jpeg" src="../../media/confusion.jpg"/>
  </media>
  <caption>
     Accuracy of Neural Network
  </caption>
</figure>
    <para id="import-auto-id1168894147333">As our neural network is a binary classification model, our output is a vector containing two entries. The first entry represents the probability of the input signal being a human speech and the second entry represents the probability of the input signal being a non-human speech. The two entries sum to one. The overall performance of our neural network is shown in the All Confusion Matrix. The red block showing 0.2% means o.2% of the class 1 examples (human speech) has been incorrectly identified as class 1 (non-human speech). Similarly, the red block showing 1.2% mean 1.2% of the examples of class 2 (non-human speech) has been incorrectly identified as class 1 (human speech). The blue block shows that our neural network is able to distinguish between human speech and non-human speech with an accuracy of 98.6%. </para>
  </content>
</document>